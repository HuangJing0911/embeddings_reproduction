{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20-fold cross-validation on all embeddings for each task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "\n",
    "import gpm\n",
    "import gpk\n",
    "\n",
    "assert np.__version__ == '1.13.1'\n",
    "assert pd.__version__ == '0.20.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_X_and_Y(df, all_X, y_column):\n",
    "    not_dropped = ~pd.isnull(df[y_column])\n",
    "    not_dropped = pd.Series(not_dropped, index=df.index)\n",
    "    Ys = df[not_dropped][y_column]\n",
    "    Ys.index = df[not_dropped]['name']\n",
    "    Xs = all_X.loc[Ys.index]\n",
    "    return Xs, Ys\n",
    "\n",
    "def score(Y, pred_Y, pred_var):\n",
    "    r1 = stats.rankdata(Y)\n",
    "    r2 = stats.rankdata(pred_Y)\n",
    "    scores = {}\n",
    "    scores['kendalltau'] = stats.kendalltau(r1, r2).correlation\n",
    "    scores['R2'] = metrics.r2_score(Y, pred_Y)\n",
    "    scores['SE'] = metrics.mean_squared_error(Y, pred_Y)\n",
    "    scores['R'] = np.corrcoef(Y, pred_Y)[0, 1]\n",
    "    log_ps = -0.5 * np.log(pred_var) - (pred_Y - Y)**2 / 2 / pred_var\n",
    "    log_ps -= 0.5 * np.log(2 * np.pi)\n",
    "    scores['log_loss'] = -np.sum(log_ps)\n",
    "    return scores\n",
    "\n",
    "def cross_validate(y_col, df, e_dir, fname):\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write('task,embedding,kernel,R,R2,kendalltau,log_loss,SE\\n')\n",
    "    for embed in os.listdir(e_dir):\n",
    "        if embed[0] != 'X':\n",
    "            continue\n",
    "        with open(e_dir + embed, 'rb') as f:\n",
    "            e_X = pickle.load(f)\n",
    "        if len(e_X) == 2:\n",
    "            e_X = e_X[0]\n",
    "        X, y = select_X_and_Y(df, e_X, y_col)\n",
    "        X = X.values\n",
    "        y = y.values\n",
    "        # Cross-validation predictions\n",
    "        kf = model_selection.KFold(n_splits=20, shuffle=True, random_state=10)\n",
    "        y_actual = []\n",
    "        mu_val = {'cubic':[], 'Matern5/2':[]}\n",
    "        var_val = {'cubic':[], 'Matern5/2':[]}\n",
    "        mu_test = {}\n",
    "        var_test = {}\n",
    "        for i_train, i_val in kf.split(X):\n",
    "            X_ = X[i_train]\n",
    "            y_ = y[i_train]\n",
    "            X_val = X[i_val]\n",
    "            y_val = y[i_val]\n",
    "            y_actual.append(y_val)\n",
    "            k = gpk.MaternKernel('5/2')\n",
    "            kernel = 'Matern5/2'\n",
    "            clf = gpm.GPRegressor(k, gueses=(10, 100))\n",
    "            clf.fit(X_, y_)\n",
    "            mu, var = clf.predict(X_val)\n",
    "            mu_val[kernel].append(mu)\n",
    "            var_val[kernel].append(np.diag(var))\n",
    "        y_actual = np.concatenate(y_actual)\n",
    "        mu_val['Matern5/2'] = np.concatenate(mu_val['Matern5/2'])\n",
    "        var_val['Matern5/2'] = np.concatenate(var_val['Matern5/2'])\n",
    "        kernels = ['Matern5/2']\n",
    "        val_scores_dict = {k:score(y_actual, mu_val[k], var_val[k]) for k in kernels}                            \n",
    "        # Write to file\n",
    "        for kernel in kernels:\n",
    "            with open(fname, 'a') as f:\n",
    "                scores = val_scores_dict[kernel]\n",
    "                f.write(','.join([y_col, embed, kernel, str(scores['R']),\n",
    "                                 str(scores['R2']), str(scores['kendalltau']),\n",
    "                                 str(scores['log_loss']), str(scores['SE'])]))\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 15min 6s, sys: 1min 21s, total: 1h 16min 27s\n",
      "Wall time: 20min 8s\n"
     ]
    }
   ],
   "source": [
    "y_col = 'log_GFP'\n",
    "dataset = '../inputs/localization.txt'\n",
    "e_dir = '../outputs/localization_embeddings/'\n",
    "fname = '../outputs/cv_localization.txt'\n",
    "\n",
    "df = pd.read_csv(dataset)\n",
    "    \n",
    "%time cross_validate(y_col, df[df.is_train], e_dir, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absorption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 54s, sys: 3.55 s, total: 5min 57s\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "y_col = 'peak'\n",
    "dataset = '../inputs/absorption.txt'\n",
    "e_dir = '../outputs/absorption_embeddings/'\n",
    "fname = '../outputs/cv_absorption.txt'\n",
    "\n",
    "df = pd.read_csv(dataset)\n",
    "    \n",
    "%time cross_validate(y_col, df[df.is_train], e_dir, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 38min 28s, sys: 1min 38s, total: 1h 40min 7s\n",
      "Wall time: 26min 21s\n"
     ]
    }
   ],
   "source": [
    "y_col = 'T50'\n",
    "dataset = '../inputs/T50.txt'\n",
    "e_dir = '../outputs/T50_embeddings/'\n",
    "fname = '../outputs/cv_T50.txt'\n",
    "\n",
    "df = pd.read_csv(dataset)\n",
    "    \n",
    "%time cross_validate(y_col, df[df.is_train], e_dir, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enantioselectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 14s, sys: 22.6 s, total: 22min 36s\n",
      "Wall time: 5min 50s\n"
     ]
    }
   ],
   "source": [
    "y_col = 'e-value'\n",
    "dataset = '../inputs/enantioselectivity.txt'\n",
    "e_dir = '../outputs/enantioselectivity_embeddings/'\n",
    "fname = '../outputs/cv_enantioselectivity.txt'\n",
    "\n",
    "df = pd.read_csv(dataset)\n",
    "    \n",
    "%time cross_validate(y_col, df[df.is_train], e_dir, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
